{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'xresned1d101' from 'src.models.xresnet1d' (c:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\src\\models\\xresnet1d.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\reproduce_results.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlit_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mptbxl_model\u001b[39;00m \u001b[39mimport\u001b[39;00m ECGClassifier\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mresnet1d\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet1d_wang\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mxresnet1d\u001b[39;00m \u001b[39mimport\u001b[39;00m xresned1d101\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mloggers\u001b[39;00m \u001b[39mimport\u001b[39;00m WandbLogger\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mptb_xl_multiclass_datamodule\u001b[39;00m \u001b[39mimport\u001b[39;00m PTB_XL_Datamodule\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'xresned1d101' from 'src.models.xresnet1d' (c:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\src\\models\\xresnet1d.py)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "from src.lit_models.ptbxl_model import ECGClassifier\n",
    "from src.models.resnet1d import resnet1d_wang\n",
    "# from src.models.xresnet1d import xresned1d101\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from src.data.ptb_xl_multiclass_datamodule import PTB_XL_Datamodule\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_registry():\n",
    "    return {\n",
    "        \"resnet1d_wang\": resnet1d_wang,\n",
    "        \"xresnet1d101\": xresned1d101\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_with_timestamp(path, prefix):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_name = f\"{prefix}_{timestamp}\"\n",
    "    full_path = os.path.join(path, dir_name)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datamodule(run, FILTER_FOR_SINGLELABEL, BATCH_SIZE):\n",
    "    artifact = run.use_artifact(f\"{'ptbxl_split'}:latest\")\n",
    "\n",
    "    datadir = artifact.download()\n",
    "\n",
    "    data_module = PTB_XL_Datamodule(Path(datadir), filter_for_singlelabel=FILTER_FOR_SINGLELABEL, batch_size=BATCH_SIZE)\n",
    "\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup()\n",
    "\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(total_optimizer_steps, model_config, model_name=\"resnet1d_wang\", loss=torch.nn.BCEWithLogitsLoss()):\n",
    "    model = get_model_registry()[model_name](\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "    model_lit = ECGClassifier(\n",
    "        model, 5, loss, 0.01, wd=0.01, total_optimizer_steps=total_optimizer_steps)\n",
    "    \n",
    "    return model_lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_lit, data_module, config):\n",
    "    wandb_logger = WandbLogger(log_model=\"all\")\n",
    "    wandb_logger.watch(model_lit, log=\"all\")\n",
    "\n",
    "    dir_model = create_directory_with_timestamp(\"./models\", \"resnet1d_wang\")\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=30, verbose=False, mode=\"min\")\n",
    "    learning_rate_monitor = LearningRateMonitor(logging_interval=\"step\", log_momentum=True)\n",
    "\n",
    "    # Create the Learner\n",
    "    trainer = pl.Trainer(\n",
    "        accumulate_grad_batches=config.ACCUMULATE_GRADIENT_STEPS,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=config.EPOCHS,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[early_stop_callback, learning_rate_monitor],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model_lit, datamodule=data_module)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(trainer, data_module, metrics={}):\n",
    "    res = trainer.predict(dataloaders=data_module.test_dataloader())\n",
    "\n",
    "    y_hat, y = torch.concatenate([x[0] for x in res]), torch.concatenate([x[1] for x in res])\n",
    "\n",
    "    y_hat = torch.nn.functional.sigmoid(y_hat)\n",
    "\n",
    "    metrics  = {\n",
    "        'multiclass_accuracy': MulticlassAccuracy(num_classes=y_hat.size(1), average='weighted')\n",
    "    }\n",
    "\n",
    "    target = torch.argmax(y, axis=-1)\n",
    "    preds = torch.argmax(y_hat, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        k: v(preds, target) for k, v in metrics.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_validation(config, project=\"ecg_benchmarking_lit\", name=\"test_run\", entity=\"phd-dk\"):\n",
    "\n",
    "    run = wandb.init(project=project, name=name, entity=entity, config=config)\n",
    "\n",
    "    BATCH_SIZE = run.config.BATCH_SIZE\n",
    "    FILTER_FOR_SINGLELABEL = run.config.FILTER_FOR_SINGLELABEL\n",
    "\n",
    "    loss = torch.nn.BCEWithLogitsLoss() if not FILTER_FOR_SINGLELABEL else torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    data_module = get_datamodule(run, FILTER_FOR_SINGLELABEL, BATCH_SIZE)\n",
    "    print(len(data_module.val_dataset))\n",
    "\n",
    "    total_optimizer_steps = int(len(data_module.train_dataset) * run.config.EPOCHS / run.config.ACCUMULATE_GRADIENT_STEPS)\n",
    "\n",
    "    model_lit = get_model(total_optimizer_steps, run.config.model_config, run.config.model_name, loss)\n",
    "\n",
    "    trainer = train_model(model_lit, data_module, run.config)\n",
    "\n",
    "\n",
    "    trainer.test(model=trainer.model, dataloaders=data_module.test_dataloader())\n",
    "\n",
    "\n",
    "\n",
    "    # results = validate_model(trainer=trainer, data_module=data_module)\n",
    "\n",
    "    # wandb_code = run.log({\n",
    "    #     f\"test/{metric_name}\": metric_value for metric_name, metric_value in results.items()\n",
    "    # })\n",
    "\n",
    "    # print(wandb_code, {\n",
    "    #     f\"test/{metric_name}\": metric_value for metric_name, metric_value in results.items()\n",
    "    # })\n",
    "\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = dict(\n",
    "    num_classes=5,\n",
    "    input_channels=12,\n",
    "    kernel_size=5,\n",
    "    ps_head=0.5,\n",
    "    lin_ftrs_head=[128],\n",
    ")\n",
    "config = {\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"EPOCHS\": 50,\n",
    "    \"ACCUMULATE_GRADIENT_STEPS\": 1,\n",
    "    \"FILTER_FOR_SINGLELABEL\" : False,\n",
    "    \"model_config\": model_config,\n",
    "    \"model_name\": \"resnet1d_wang\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33markadiusz-czerwinski\u001b[0m (\u001b[33mphd-dk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\wandb\\run-20231014_192502-8n7kvilb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/8n7kvilb' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/8n7kvilb' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/8n7kvilb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ptbxl_split:latest, 1800.85MB. 9 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   9 of 9 files downloaded.  \n",
      "Done. 0:0:2.6\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name                   | Type                       | Params\n",
      "-----------------------------------------------------------------------\n",
      "0  | model                  | ResNet1d                   | 473 K \n",
      "1  | loss_fn                | BCEWithLogitsLoss          | 0     \n",
      "2  | train_accuracy         | MulticlassAccuracy         | 0     \n",
      "3  | train_auroc            | MulticlassAUROC            | 0     \n",
      "4  | train_averageprecision | MulticlassAveragePrecision | 0     \n",
      "5  | train_f1score          | MulticlassF1Score          | 0     \n",
      "6  | train_matthewscorrcoef | MulticlassMatthewsCorrCoef | 0     \n",
      "7  | train_precision        | MulticlassPrecision        | 0     \n",
      "8  | train_recall           | MulticlassRecall           | 0     \n",
      "9  | train_specificity      | MulticlassSpecificity      | 0     \n",
      "10 | val_accuracy           | MulticlassAccuracy         | 0     \n",
      "11 | val_auroc              | MulticlassAUROC            | 0     \n",
      "12 | val_averageprecision   | MulticlassAveragePrecision | 0     \n",
      "13 | val_f1score            | MulticlassF1Score          | 0     \n",
      "14 | val_matthewscorrcoef   | MulticlassMatthewsCorrCoef | 0     \n",
      "15 | val_precision          | MulticlassPrecision        | 0     \n",
      "16 | val_recall             | MulticlassRecall           | 0     \n",
      "17 | val_specificity        | MulticlassSpecificity      | 0     \n",
      "18 | test_accuracy          | MulticlassAccuracy         | 0     \n",
      "19 | test_auroc             | MulticlassAUROC            | 0     \n",
      "20 | test_averageprecision  | MulticlassAveragePrecision | 0     \n",
      "21 | test_f1score           | MulticlassF1Score          | 0     \n",
      "22 | test_matthewscorrcoef  | MulticlassMatthewsCorrCoef | 0     \n",
      "23 | test_precision         | MulticlassPrecision        | 0     \n",
      "24 | test_recall            | MulticlassRecall           | 0     \n",
      "25 | test_specificity       | MulticlassSpecificity      | 0     \n",
      "-----------------------------------------------------------------------\n",
      "473 K     Trainable params\n",
      "0         Non-trainable params\n",
      "473 K     Total params\n",
      "1.893     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 117/117 [00:11<00:00,  9.93it/s, v_num=vilb, train_loss_step=0.194, val_loss_step=0.478, val_loss_epoch=0.236, train_loss_epoch=0.191]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 117/117 [00:12<00:00,  9.48it/s, v_num=vilb, train_loss_step=0.194, val_loss_step=0.478, val_loss_epoch=0.236, train_loss_epoch=0.191]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   3%|▎         | 3/118 [00:00<00:11, 10.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 118/118 [00:04<00:00, 24.52it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric               DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    test_accuracy_epoch        0.6889894604682922\n",
      "     test_auroc_epoch          0.8707942962646484\n",
      "test_averageprecision_epoch    0.5935240983963013\n",
      "    test_f1score_epoch         0.6889894604682922\n",
      "test_matthewscorrcoef_epoch    0.5619130730628967\n",
      "   test_precision_epoch        0.6889894604682922\n",
      "     test_recall_epoch         0.6889894604682922\n",
      "  test_specificity_epoch       0.9222473502159119\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-AdamW</td><td>▁▁▂▂▃▄▅▆▆▇███████▇▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>lr-AdamW-momentum</td><td>██▇▇▆▅▄▃▃▂▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>test_accuracy_epoch</td><td>▁</td></tr><tr><td>test_accuracy_step</td><td>▄▆▇▄▄▆▇▆▅▁▂▅▄▅▃▅▂▆█▃▄▆▆▂▄▄▅▂▅▆▅▅▇▇▇▅▆▇▄▄</td></tr><tr><td>test_auroc_epoch</td><td>▁</td></tr><tr><td>test_auroc_step</td><td>█▅▆█▇▆▆█▆▆▇▅█▃▆▆▇▅▆▃▆▃▅▆▄▅▃▅█▃▅▅▆▅▃█▇▅▅▁</td></tr><tr><td>test_averageprecision_epoch</td><td>▁</td></tr><tr><td>test_averageprecision_step</td><td>▅▆▆▄▆▅▆▅▇▂▃▄▅▄▂▂▄▃▇▃▃▆▅▃▃▆▆▁▆▄▅▅▇▄█▆▄▅▄█</td></tr><tr><td>test_f1score_epoch</td><td>▁</td></tr><tr><td>test_f1score_step</td><td>▄▆▇▄▄▆▇▆▅▁▂▅▄▅▃▅▂▆█▃▄▆▆▂▄▄▅▂▅▆▅▅▇▇▇▅▆▇▄▄</td></tr><tr><td>test_matthewscorrcoef_epoch</td><td>▁</td></tr><tr><td>test_matthewscorrcoef_step</td><td>▃▅▆▅▄▅▇▆▃▁▂▅▅▄▃▅▂▆█▃▄▅▆▂▃▄▅▁▅▅▅▅▇▇▇▅▅▇▃▅</td></tr><tr><td>test_precision_epoch</td><td>▁</td></tr><tr><td>test_precision_step</td><td>▄▆▇▄▄▆▇▆▅▁▂▅▄▅▃▅▂▆█▃▄▆▆▂▄▄▅▂▅▆▅▅▇▇▇▅▆▇▄▄</td></tr><tr><td>test_recall_epoch</td><td>▁</td></tr><tr><td>test_recall_step</td><td>▄▆▇▄▄▆▇▆▅▁▂▅▄▅▃▅▂▆█▃▄▆▆▂▄▄▅▂▅▆▅▅▇▇▇▅▆▇▄▄</td></tr><tr><td>test_specificity_epoch</td><td>▁</td></tr><tr><td>test_specificity_step</td><td>▄▆▇▄▄▆▇▆▅▁▂▅▄▅▃▅▂▆█▃▄▆▆▂▄▄▅▂▅▆▅▅▇▇▇▅▆▇▄▄</td></tr><tr><td>train_accuracy_epoch</td><td>▁▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_accuracy_step</td><td>▁▁▂▂▄▄▃▄▄▅▅▄▅▅▄▆▅▃▅█▂▃▇▅▆▅▅▄▅▅▄▃▇▄▇▇▇▇█▇</td></tr><tr><td>train_auroc_epoch</td><td>▁▃▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>train_auroc_step</td><td>▃▅▁▄▄▇▆▇▆▆▇▆▇▇▇▅▇▄▆▇▆▆▇▆█▇▇▇▇▇▆▇█▆█▇██▇█</td></tr><tr><td>train_averageprecision_epoch</td><td>▁▃▃▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train_averageprecision_step</td><td>▁▄▃▂▃▄▄▆▃▅▅▄▄▆▆▃▇▂▃▆▄▃▅▄▇▆▅▅▄▆▄▅█▅█▅█▇▅█</td></tr><tr><td>train_f1score_epoch</td><td>▁▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_f1score_step</td><td>▁▁▂▂▄▄▃▄▄▅▅▄▅▅▄▆▅▃▅█▂▃▇▅▆▅▅▄▅▅▄▃▇▄▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▄▃▃▄▄▃▄▃▃▃▃▂▂▃▃▂▄▄▂▃▂▂▂▁▂▃▃▂▂▂▂▂▁▂▁▁</td></tr><tr><td>train_matthewscorrcoef_epoch</td><td>▁▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_matthewscorrcoef_step</td><td>▁▁▁▂▄▄▃▄▄▅▅▄▅▅▄▆▅▂▅█▃▃▇▄▆▅▅▄▅▅▄▃█▄▇▇▇▇█▇</td></tr><tr><td>train_precision_epoch</td><td>▁▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_precision_step</td><td>▁▁▂▂▄▄▃▄▄▅▅▄▅▅▄▆▅▃▅█▂▃▇▅▆▅▅▄▅▅▄▃▇▄▇▇▇▇█▇</td></tr><tr><td>train_recall_epoch</td><td>▁▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_recall_step</td><td>▁▁▂▂▄▄▃▄▄▅▅▄▅▅▄▆▅▃▅█▂▃▇▅▆▅▅▄▅▅▄▃▇▄▇▇▇▇█▇</td></tr><tr><td>train_specificity_epoch</td><td>▁▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_specificity_step</td><td>▁▁▂▂▄▄▃▄▄▅▅▄▅▅▄▆▅▃▅█▂▃▇▅▆▅▅▄▅▅▄▃▇▄▇▇▇▇█▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███▁</td></tr><tr><td>val_accuracy_epoch</td><td>▂▄▂▁▆▅▅▆▇▆▄▆▆▆▇▆▆▆▇▇▇█▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>val_accuracy_step</td><td>▂▆▂▃▄▃▅▂▃▆▄▆▄▇▂▇▄▆▅▃█▄▁▄▄▅▇▁▆▅▁▆▄▁▆▆▂▄▅▄</td></tr><tr><td>val_auroc_epoch</td><td>▁▁▃▃▅▅▅▆▆▆▆▆▆▇▇▅▇▆▆▇██▇█▇▇▇▇██████▇█████</td></tr><tr><td>val_auroc_step</td><td>▅▅▄██▇▅█▅▃██▃▃▆▆▇█▂▆▆▃▆▇▆█▆▄▆▄▁▅▇▅▇▃▆▃█▅</td></tr><tr><td>val_averageprecision_epoch</td><td>▁▂▂▃▄▃▄▄▄▆▅▆▅▅▆▄▆▆▆▆▇▇▇█▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_averageprecision_step</td><td>▅▄▃▆▅▄▄▄▄▆▆▆▆█▁█▄▇▆▄█▃▂▆▃▅▆▂█▃▁▆▅▆▅▇▃▆▇▆</td></tr><tr><td>val_f1score_epoch</td><td>▂▄▂▁▆▅▅▆▇▆▄▆▆▆▇▆▆▆▇▇▇█▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>val_f1score_step</td><td>▂▆▂▃▄▃▅▂▃▆▄▆▄▇▂▇▄▆▅▃█▄▁▄▄▅▇▁▆▅▁▆▄▁▆▆▂▄▅▄</td></tr><tr><td>val_loss_epoch</td><td>█▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▃▇▅▄▇▃▅▃▃▃▃▃▂▅▃▄▄▂▄▁▆▄▄▆▃▁▆▂▂▄▃▃▄▁▂▇▄▄▄</td></tr><tr><td>val_matthewscorrcoef_epoch</td><td>▁▂▁▁▅▄▅▅▆▅▄▅▆▅▆▆▅▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇██▇█████</td></tr><tr><td>val_matthewscorrcoef_step</td><td>▄▆▄▅▆▅▆▄▅▇▆▇▆▇▄█▅▆▆▅█▄▃▅▅▆▇▃▆▅▁▆▅▄▇▆▄▅▆▅</td></tr><tr><td>val_precision_epoch</td><td>▂▄▂▁▆▅▅▆▇▆▄▆▆▆▇▆▆▆▇▇▇█▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>val_precision_step</td><td>▂▆▂▃▄▃▅▂▃▆▄▆▄▇▂▇▄▆▅▃█▄▁▄▄▅▇▁▆▅▁▆▄▁▆▆▂▄▅▄</td></tr><tr><td>val_recall_epoch</td><td>▂▄▂▁▆▅▅▆▇▆▄▆▆▆▇▆▆▆▇▇▇█▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>val_recall_step</td><td>▂▆▂▃▄▃▅▂▃▆▄▆▄▇▂▇▄▆▅▃█▄▁▄▄▅▇▁▆▅▁▆▄▁▆▆▂▄▅▄</td></tr><tr><td>val_specificity_epoch</td><td>▂▄▂▁▆▅▅▆▇▆▄▆▆▆▇▆▆▆▇▇▇█▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>val_specificity_step</td><td>▂▆▂▃▄▃▅▂▃▆▄▆▄▇▂▇▄▆▅▃█▄▁▄▄▅▇▁▆▅▁▆▄▁▆▆▂▄▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>lr-AdamW-momentum</td><td>0.95</td></tr><tr><td>test_accuracy_epoch</td><td>0.68899</td></tr><tr><td>test_accuracy_step</td><td>0.63043</td></tr><tr><td>test_auroc_epoch</td><td>0.87079</td></tr><tr><td>test_auroc_step</td><td>0.4</td></tr><tr><td>test_averageprecision_epoch</td><td>0.59352</td></tr><tr><td>test_averageprecision_step</td><td>1.0</td></tr><tr><td>test_f1score_epoch</td><td>0.68899</td></tr><tr><td>test_f1score_step</td><td>0.63043</td></tr><tr><td>test_matthewscorrcoef_epoch</td><td>0.56191</td></tr><tr><td>test_matthewscorrcoef_step</td><td>0.56086</td></tr><tr><td>test_precision_epoch</td><td>0.68899</td></tr><tr><td>test_precision_step</td><td>0.63043</td></tr><tr><td>test_recall_epoch</td><td>0.68899</td></tr><tr><td>test_recall_step</td><td>0.63043</td></tr><tr><td>test_specificity_epoch</td><td>0.92225</td></tr><tr><td>test_specificity_step</td><td>0.90761</td></tr><tr><td>train_accuracy_epoch</td><td>0.76744</td></tr><tr><td>train_accuracy_step</td><td>0.7757</td></tr><tr><td>train_auroc_epoch</td><td>0.92086</td></tr><tr><td>train_auroc_step</td><td>0.9076</td></tr><tr><td>train_averageprecision_epoch</td><td>0.69787</td></tr><tr><td>train_averageprecision_step</td><td>0.74782</td></tr><tr><td>train_f1score_epoch</td><td>0.76744</td></tr><tr><td>train_f1score_step</td><td>0.7757</td></tr><tr><td>train_loss_epoch</td><td>0.19104</td></tr><tr><td>train_loss_step</td><td>0.19433</td></tr><tr><td>train_matthewscorrcoef_epoch</td><td>0.67655</td></tr><tr><td>train_matthewscorrcoef_step</td><td>0.69085</td></tr><tr><td>train_precision_epoch</td><td>0.76744</td></tr><tr><td>train_precision_step</td><td>0.7757</td></tr><tr><td>train_recall_epoch</td><td>0.76744</td></tr><tr><td>train_recall_step</td><td>0.7757</td></tr><tr><td>train_specificity_epoch</td><td>0.94186</td></tr><tr><td>train_specificity_step</td><td>0.94393</td></tr><tr><td>trainer/global_step</td><td>4680</td></tr><tr><td>val_accuracy_epoch</td><td>0.74254</td></tr><tr><td>val_accuracy_step</td><td>0.87273</td></tr><tr><td>val_auroc_epoch</td><td>0.90721</td></tr><tr><td>val_auroc_step</td><td>0.45158</td></tr><tr><td>val_averageprecision_epoch</td><td>0.68464</td></tr><tr><td>val_averageprecision_step</td><td>0.66504</td></tr><tr><td>val_f1score_epoch</td><td>0.74254</td></tr><tr><td>val_f1score_step</td><td>0.87273</td></tr><tr><td>val_loss_epoch</td><td>0.23564</td></tr><tr><td>val_loss_step</td><td>0.47796</td></tr><tr><td>val_matthewscorrcoef_epoch</td><td>0.64177</td></tr><tr><td>val_matthewscorrcoef_step</td><td>0.80062</td></tr><tr><td>val_precision_epoch</td><td>0.74254</td></tr><tr><td>val_precision_step</td><td>0.87273</td></tr><tr><td>val_recall_epoch</td><td>0.74254</td></tr><tr><td>val_recall_step</td><td>0.87273</td></tr><tr><td>val_specificity_epoch</td><td>0.93563</td></tr><tr><td>val_specificity_step</td><td>0.96818</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_run</strong> at: <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/8n7kvilb' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/8n7kvilb</a><br/>Synced 6 W&B file(s), 1 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231014_192502-8n7kvilb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model_with_validation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
