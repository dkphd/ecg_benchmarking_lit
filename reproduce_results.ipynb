{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from src.lit_models.ptbxl_model import ECGClassifier\n",
    "from src.models.resnet1d import resnet1d_wang\n",
    "from src.models.inception1d import inception1d\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from src.data.ptb_xl_multiclass_datamodule import PTB_XL_Datamodule\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_registry():\n",
    "    return {\n",
    "        \"resnet1d_wang\": resnet1d_wang,\n",
    "        \"inception1d\": inception1d\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_with_timestamp(path, prefix):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_name = f\"{prefix}_{timestamp}\"\n",
    "    full_path = os.path.join(path, dir_name)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datamodule(run, FILTER_FOR_SINGLELABEL, BATCH_SIZE):\n",
    "    artifact = run.use_artifact(f\"{'ptbxl_split'}:latest\")\n",
    "\n",
    "    datadir = artifact.download()\n",
    "\n",
    "    data_module = PTB_XL_Datamodule(Path(datadir), filter_for_singlelabel=FILTER_FOR_SINGLELABEL, batch_size=BATCH_SIZE)\n",
    "\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config, model_name=\"resnet1d_wang\", task='multilabel', loss=torch.nn.BCEWithLogitsLoss()):\n",
    "    model = get_model_registry()[model_name](\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "    model_lit = ECGClassifier(\n",
    "        model, 5, loss, 0.01, wd=0.01, task=task)\n",
    "    \n",
    "    return model_lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_lit, data_module, config):\n",
    "    wandb_logger = WandbLogger(log_model=\"all\")\n",
    "\n",
    "    dir_model = create_directory_with_timestamp(\"./models\", \"resnet1d_wang\")\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=30, verbose=False, mode=\"min\")\n",
    "    learning_rate_monitor = LearningRateMonitor(logging_interval=\"step\", log_momentum=True)\n",
    "    model_checkpoint = ModelCheckpoint(monitor='val_loss', save_top_k=3, save_last=True, mode='min')\n",
    "\n",
    "    # Create the Learner\n",
    "    trainer = pl.Trainer(\n",
    "        accumulate_grad_batches=config.ACCUMULATE_GRADIENT_STEPS,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=config.EPOCHS,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[early_stop_callback, learning_rate_monitor, model_checkpoint],\n",
    "        accelerator=\"gpu\"\n",
    "    )\n",
    "\n",
    "    wandb_logger.watch(model_lit, log=\"all\")\n",
    "\n",
    "    trainer.fit(model_lit, datamodule=data_module)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_validation(config, project=\"ecg_benchmarking_lit\", name=None, entity=\"phd-dk\"):\n",
    "\n",
    "    run = wandb.init(project=project, name=name, entity=entity, config=config)\n",
    "\n",
    "    BATCH_SIZE = run.config.BATCH_SIZE\n",
    "    FILTER_FOR_SINGLELABEL = run.config.FILTER_FOR_SINGLELABEL\n",
    "\n",
    "    loss = torch.nn.BCEWithLogitsLoss() if not FILTER_FOR_SINGLELABEL else torch.nn.CrossEntropyLoss()\n",
    "    task = \"multilabel\" if not FILTER_FOR_SINGLELABEL else \"multiclass\"\n",
    "\n",
    "    data_module = get_datamodule(run, FILTER_FOR_SINGLELABEL, BATCH_SIZE)\n",
    "\n",
    "    model_lit = get_model(run.config.model_config, run.config.model_name, task, loss)\n",
    "\n",
    "    trainer = train_model(model_lit, data_module, run.config)\n",
    "\n",
    "\n",
    "    trainer.test(model=trainer.model, dataloaders=data_module.test_dataloader())\n",
    "\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    return trainer, data_module, model_lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = dict(\n",
    "    num_classes=5,\n",
    "    input_channels=12,\n",
    "    kernel_size=5 * 8,\n",
    "    ps_head=0.5,\n",
    "    lin_ftrs_head=[128],\n",
    ")\n",
    "config = {\n",
    "    \"BATCH_SIZE\": 128,\n",
    "    \"EPOCHS\": 50,\n",
    "    \"ACCUMULATE_GRADIENT_STEPS\": 1,\n",
    "    \"FILTER_FOR_SINGLELABEL\" : False,\n",
    "    \"model_config\": model_config,\n",
    "    \"model_name\": \"inception1d\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer, data_module, model = train_model_with_validation(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
