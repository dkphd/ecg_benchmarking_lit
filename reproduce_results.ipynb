{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "\n",
    "from src.lit_models.ptbxl_model import ECGClassifier\n",
    "from src.models.resnet1d import resnet1d_wang\n",
    "from src.models.conv_transformer import conv_transformer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from src.data.ptb_xl_multiclass_datamodule import PTB_XL_Datamodule\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_registry():\n",
    "    return {\n",
    "        \"resnet1d_wang\": resnet1d_wang,\n",
    "        \"conv_transformer\": conv_transformer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_with_timestamp(path, prefix):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_name = f\"{prefix}_{timestamp}\"\n",
    "    full_path = os.path.join(path, dir_name)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    return full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datamodule(run, FILTER_FOR_SINGLELABEL, BATCH_SIZE):\n",
    "    artifact = run.use_artifact(f\"{'ptbxl_split'}:latest\")\n",
    "\n",
    "    datadir = artifact.download()\n",
    "\n",
    "    data_module = PTB_XL_Datamodule(Path(datadir), filter_for_singlelabel=FILTER_FOR_SINGLELABEL, batch_size=BATCH_SIZE)\n",
    "\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup()\n",
    "\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(total_optimizer_steps, model_config, model_name=\"resnet1d_wang\", task='multilabel', loss=torch.nn.BCEWithLogitsLoss()):\n",
    "    model = get_model_registry()[model_name](\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "    model_lit = ECGClassifier(\n",
    "        model, 5, loss, 0.01, wd=0.01, total_optimizer_steps=total_optimizer_steps, task=task)\n",
    "    \n",
    "    return model_lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_lit, data_module, config):\n",
    "    wandb_logger = WandbLogger(log_model=\"all\")\n",
    "    wandb_logger.watch(model_lit, log=\"all\")\n",
    "\n",
    "    dir_model = create_directory_with_timestamp(\"./models\", \"resnet1d_wang\")\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=30, verbose=False, mode=\"min\")\n",
    "    learning_rate_monitor = LearningRateMonitor(logging_interval=\"step\", log_momentum=True)\n",
    "\n",
    "    # Create the Learner\n",
    "    trainer = pl.Trainer(\n",
    "        accumulate_grad_batches=config.ACCUMULATE_GRADIENT_STEPS,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=config.EPOCHS,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[early_stop_callback, learning_rate_monitor],\n",
    "    )\n",
    "\n",
    "    trainer.fit(model_lit, datamodule=data_module)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(trainer, data_module, metrics={}):\n",
    "    res = trainer.predict(dataloaders=data_module.test_dataloader())\n",
    "\n",
    "    y_hat, y = torch.concatenate([x[0] for x in res]), torch.concatenate([x[1] for x in res])\n",
    "\n",
    "    y_hat = torch.nn.functional.sigmoid(y_hat)\n",
    "\n",
    "    metrics  = {\n",
    "        'multiclass_accuracy': MulticlassAccuracy(num_classes=y_hat.size(1), average='weighted')\n",
    "    }\n",
    "\n",
    "    target = torch.argmax(y, axis=-1)\n",
    "    preds = torch.argmax(y_hat, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        k: v(preds, target) for k, v in metrics.items()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_with_validation(config, project=\"ecg_benchmarking_lit\", name=\"test_run\", entity=\"phd-dk\"):\n",
    "\n",
    "    run = wandb.init(project=project, name=name, entity=entity, config=config)\n",
    "\n",
    "    BATCH_SIZE = run.config.BATCH_SIZE\n",
    "    FILTER_FOR_SINGLELABEL = run.config.FILTER_FOR_SINGLELABEL\n",
    "\n",
    "    loss = torch.nn.BCEWithLogitsLoss() if not FILTER_FOR_SINGLELABEL else torch.nn.CrossEntropyLoss()\n",
    "    task = \"multilabel\" if not FILTER_FOR_SINGLELABEL else \"multiclass\"\n",
    "\n",
    "    data_module = get_datamodule(run, FILTER_FOR_SINGLELABEL, BATCH_SIZE)\n",
    "    print(len(data_module.val_dataset))\n",
    "\n",
    "    total_optimizer_steps = int(len(data_module.train_dataset) * run.config.EPOCHS / run.config.ACCUMULATE_GRADIENT_STEPS)\n",
    "\n",
    "    model_lit = get_model(total_optimizer_steps, run.config.model_config, run.config.model_name, task, loss)\n",
    "\n",
    "    trainer = train_model(model_lit, data_module, run.config)\n",
    "\n",
    "\n",
    "    trainer.test(model=trainer.model, dataloaders=data_module.test_dataloader())\n",
    "\n",
    "\n",
    "\n",
    "    # results = validate_model(trainer=trainer, data_module=data_module)\n",
    "\n",
    "    # wandb_code = run.log({\n",
    "    #     f\"test/{metric_name}\": metric_value for metric_name, metric_value in results.items()\n",
    "    # })\n",
    "\n",
    "    # print(wandb_code, {\n",
    "    #     f\"test/{metric_name}\": metric_value for metric_name, metric_value in results.items()\n",
    "    # })\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    return trainer, data_module, model_lit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = dict(\n",
    "    k = 12,\n",
    "    headers = 10,\n",
    "    depth = 5,\n",
    "    seq_length= 128\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"BATCH_SIZE\": 10,\n",
    "    \"EPOCHS\": 50,\n",
    "    \"ACCUMULATE_GRADIENT_STEPS\": 1,\n",
    "    \"FILTER_FOR_SINGLELABEL\" : False,\n",
    "    \"model_config\": model_config,\n",
    "    \"model_name\": \"conv_transformer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33markadiusz-czerwinski\u001b[0m (\u001b[33mphd-dk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\wandb\\run-20231015_140646-0w7dxyvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/0w7dxyvr' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/0w7dxyvr' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/0w7dxyvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ptbxl_split:latest, 1800.85MB. 9 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   9 of 9 files downloaded.  \n",
      "Done. 0:0:3.5\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name                   | Type                       | Params\n",
      "-----------------------------------------------------------------------\n",
      "0  | model                  | ResNet1d                   | 473 K \n",
      "1  | loss_fn                | BCEWithLogitsLoss          | 0     \n",
      "2  | train_accuracy         | MultilabelAccuracy         | 0     \n",
      "3  | train_auroc            | MultilabelAUROC            | 0     \n",
      "4  | train_averageprecision | MultilabelAveragePrecision | 0     \n",
      "5  | train_f1score          | MultilabelF1Score          | 0     \n",
      "6  | train_matthewscorrcoef | MultilabelMatthewsCorrCoef | 0     \n",
      "7  | train_precision        | MultilabelPrecision        | 0     \n",
      "8  | train_recall           | MultilabelRecall           | 0     \n",
      "9  | train_specificity      | MultilabelSpecificity      | 0     \n",
      "10 | val_accuracy           | MultilabelAccuracy         | 0     \n",
      "11 | val_auroc              | MultilabelAUROC            | 0     \n",
      "12 | val_averageprecision   | MultilabelAveragePrecision | 0     \n",
      "13 | val_f1score            | MultilabelF1Score          | 0     \n",
      "14 | val_matthewscorrcoef   | MultilabelMatthewsCorrCoef | 0     \n",
      "15 | val_precision          | MultilabelPrecision        | 0     \n",
      "16 | val_recall             | MultilabelRecall           | 0     \n",
      "17 | val_specificity        | MultilabelSpecificity      | 0     \n",
      "18 | test_accuracy          | MultilabelAccuracy         | 0     \n",
      "19 | test_auroc             | MultilabelAUROC            | 0     \n",
      "20 | test_averageprecision  | MultilabelAveragePrecision | 0     \n",
      "21 | test_f1score           | MultilabelF1Score          | 0     \n",
      "22 | test_matthewscorrcoef  | MultilabelMatthewsCorrCoef | 0     \n",
      "23 | test_precision         | MultilabelPrecision        | 0     \n",
      "24 | test_recall            | MultilabelRecall           | 0     \n",
      "25 | test_specificity       | MultilabelSpecificity      | 0     \n",
      "-----------------------------------------------------------------------\n",
      "473 K     Trainable params\n",
      "0         Non-trainable params\n",
      "473 K     Total params\n",
      "1.893     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:04<00:00,  0.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34:   1%|          | 1/117 [00:00<00:08, 13.00it/s, v_num=xyvr, train_loss_step=0.214, val_loss_step=0.466, val_loss_epoch=0.241, train_loss_epoch=0.226]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   9%|▉         | 11/118 [00:00<00:06, 15.62it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 118/118 [00:05<00:00, 21.40it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test metric               DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    test_accuracy_epoch        0.8827852606773376\n",
      "     test_auroc_epoch          0.9295147061347961\n",
      "test_averageprecision_epoch    0.8131654858589172\n",
      "    test_f1score_epoch         0.7669419646263123\n",
      "test_matthewscorrcoef_epoch    0.6894055604934692\n",
      "   test_precision_epoch        0.7927109599113464\n",
      "     test_recall_epoch         0.7427955865859985\n",
      "  test_specificity_epoch       0.9318802952766418\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer, data_module, model = train_model_with_validation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33markadiusz-czerwinski\u001b[0m (\u001b[33mavatar2pjm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\wandb\\run-20231014_211521-28g56i1r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/28g56i1r' target=\"_blank\">soft-firebrand-132</a></strong> to <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/28g56i1r' target=\"_blank\">https://wandb.ai/phd-dk/ecg_benchmarking_lit/runs/28g56i1r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('phd-dk/ecg_benchmarking_lit/model-ied6sv54:v4', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "torchmetrics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\artifacts\\\\model-ied6sv54-v4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x24f62db07f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.data.ptb_xl_multiclass_datamodule.PTB_XL_Datamodule at 0x24f60287850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import F1Score\n",
    "from torchmetrics.classification import MulticlassF1Score, MultilabelF1Score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(trainer, loader, metrics={}):\n",
    "    res = trainer.predict(dataloaders=loader)\n",
    "\n",
    "    y_hat, y = torch.concatenate([x[0] for x in res]), torch.concatenate([x[1] for x in res])\n",
    "\n",
    "    y_hat = torch.nn.functional.sigmoid(y_hat)\n",
    "\n",
    "    metrics  = {\n",
    "        'multiclass_accuracy': MulticlassAccuracy(num_classes=y_hat.size(1), average='weighted')\n",
    "    }\n",
    "\n",
    "    # metric = F1Score(num_labels=5, task='multilabel')\n",
    "\n",
    "    metric = f1_score\n",
    "\n",
    "    result = metric(y_hat, y, average='micro')\n",
    "\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:145: `.predict(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.predict(ckpt_path='best')` to use the best model or `.predict(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at .\\lightning_logs\\phun1fnc\\checkpoints\\epoch=49-step=5850.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at .\\lightning_logs\\phun1fnc\\checkpoints\\epoch=49-step=5850.ckpt\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 118/118 [00:01<00:00, 59.83it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\reproduce_results.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m validate_model(trainer, data_module\u001b[39m.\u001b[39;49mtest_dataloader())\n",
      "\u001b[1;32mc:\\Users\\arekp\\OneDrive\\Desktop\\ecg_benchmarking_lit\\reproduce_results.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# metric = F1Score(num_labels=5, task='multilabel')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m metric \u001b[39m=\u001b[39m f1_score\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m result \u001b[39m=\u001b[39m metric(y_hat, y, average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmicro\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arekp/OneDrive/Desktop/ecg_benchmarking_lit/reproduce_results.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m   1071\u001b[0m     {\n\u001b[0;32m   1072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1097\u001b[0m ):\n\u001b[0;32m   1098\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \n\u001b[0;32m   1100\u001b[0m \u001b[39m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[39mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1239\u001b[0m         y_true,\n\u001b[0;32m   1240\u001b[0m         y_pred,\n\u001b[0;32m   1241\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1242\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1243\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1244\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1245\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1246\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1247\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[0;32m   1251\u001b[0m     {\n\u001b[0;32m   1252\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msparse matrix\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m ):\n\u001b[0;32m   1280\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \n\u001b[0;32m   1282\u001b[0m \u001b[39m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39m    0.38...\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1412\u001b[0m         y_true,\n\u001b[0;32m   1413\u001b[0m         y_pred,\n\u001b[0;32m   1414\u001b[0m         beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[0;32m   1415\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1416\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1417\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1418\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1419\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1420\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1421\u001b[0m     )\n\u001b[0;32m   1422\u001b[0m     \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[39m=\u001b[39m get_config()[\u001b[39m\"\u001b[39m\u001b[39mskip_parameter_validation\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[39m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[39m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m \n\u001b[0;32m   1565\u001b[0m \u001b[39mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[39m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m zero_division_value \u001b[39m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1721\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1723\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1499\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1499\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1500\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and multilabel-indicator targets"
     ]
    }
   ],
   "source": [
    "validate_model(trainer, data_module.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:145: `.predict(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.predict(ckpt_path='best')` to use the best model or `.predict(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at .\\lightning_logs\\phun1fnc\\checkpoints\\epoch=49-step=5850.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at .\\lightning_logs\\phun1fnc\\checkpoints\\epoch=49-step=5850.ckpt\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:492: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 117/117 [00:01<00:00, 59.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8790)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(trainer, data_module.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:145: `.predict(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.predict(ckpt_path='best')` to use the best model or `.predict(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at .\\lightning_logs\\phun1fnc\\checkpoints\\epoch=49-step=5850.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at .\\lightning_logs\\phun1fnc\\checkpoints\\epoch=49-step=5850.ckpt\n",
      "c:\\Users\\arekp\\anaconda3\\envs\\phd\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 117/117 [00:01<00:00, 62.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8039)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(trainer, data_module.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
